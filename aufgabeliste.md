Aufgabenliste: Tensor â†’ Quantenzustand (End-to-End)
1) Tensor exakt spezifizieren

1.1 Spezifikations-Checkliste (ausfÃ¼llen)
- Zweck/Use-Case: WofÃ¼r wird T verwendet (Encoding, Training, Inferenz)?
- Datentyp/PrÃ¤zision: {float32 | float64 | int8 | uint8 | complex64 â€¦}
- Wertebereich & Einheiten: min/max, Skala, Offset, erlaubte Sonderwerte (NaN/Inf?)
- Form/Shape: {d | HÃ—WÃ—C | â€¦} und feste GrÃ¶ÃŸen vs. variabel; Batch separat halten
- Ordnung/Indexierung: {row-major | col-major}; klare i â†” Koordinate-Abbildung
- Reell/komplex: Reellwerte, oder getrennte Real/Imag/Phase-Kodierung
- Normalisierung: mean/std oder L2-Norm (bes. fÃ¼r Amplituden-Encoding Pflicht)
- Quantisierung/Clipping: Ja/Nein; Bitbreite, Schwellen, Rundungsmodus
- Padding-Regeln: auf 2^n fÃ¼r Amplituden; Pad-Wert, MaskenfÃ¼hrung
- Ziel (fÃ¼r Schritt 5): {|ÏˆâŸ© Amplituden/Phasen | Parameter Î˜ (Winkel)}
- Observablen-Vorplanung: welche Messoperatoren sind relevant (Z, ZZ, Pauli-Strings)?
- Metadaten/Versionierung: Spec-ID, Datum, Autor, Datensatz-Version

1.2 Akzeptanzkriterien (automatisierbar)
- Shape stimmt exakt mit Spezifikation Ã¼berein
- dtype stimmt; keine impliziten Up/Downcasts beim Laden
- Wertebereich eingehalten; optional assert min/max
- Keine NaN/Inf nach Vorverarbeitung
- FÃ¼r Amplituden-Encoding: ||x||2 = 1 Â± 1e-7 (nach Padding & Normierung)
- FÃ¼r Angle/Phase: Winkel liegen innerhalb definierter Grenzen (z. B. [âˆ’Ï€, Ï€])
- Indexierungsrunde: Stichprobe i â†” Koordinate ergibt erwartete Position

1.3 Vorlage (zum AusfÃ¼llen)
```yaml
tensor_spec:
	id: qnn.tensor.v1
	purpose: <z. B. Input-Features fÃ¼r Angle-Encoding>
	dtype: float32
	value_range:
		min: -1.0
		max: 1.0
		units: normalized
		allow_nan_inf: false
	shape:
		layout: HxWxC
		H: <int>
		W: <int>
		C: <int>
		batch_dimension: separate
	indexing:
		order: row-major
		mapping_note: "i â†” (h,w,c) with i = ((h*W)+w)*C + c"
	complex:
		representation: real
	preprocessing:
		normalize: {type: none | l2 | zscore, params: {}}
		clip: {enabled: false, min: null, max: null}
		quantize: {enabled: false, bits: null, mode: null}
		pad:
			to_power_of_two: {enabled: false, axis: flattened, pad_value: 0.0}
	encoding_goal:
		target: angle  # angle | amplitude | phase | basis
		angles:
			alpha: <float>  # Î¸ = Î± x, keep within [âˆ’Ï€, Ï€]
		amplitude:
			require_l2_norm: true
		phase:
			beta: <float>
	observables_plan:
		- name: Z_chain
			desc: ZâŠ—â€¦âŠ—Z on data qubits
	versioning:
		date: 2025-08-26
		author: <name>
		dataset_version: <id>
```

1.4 Beispiele (Richtwerte)
- Beispiel A (Bild 28Ã—28Ã—1, Angle-Encoding): dtype=float32, Range [0,1] â†’ rescale zu [âˆ’1,1]; Î±=Ï€ so dass Î¸ âˆˆ [âˆ’Ï€, Ï€]; row-major Flatten, i = hÂ·WÂ·C + wÂ·C + c.
- Beispiel B (Vektor d=1000, Amplituden-Encoding): pad auf dâ€™=1024, n=10 Qubits; L2-Normierung strikt, pad mit 0, Maske m fÃ¼r erste 1000 Indizes speichern.

Form: 
ð‘‡
âˆˆ
ð‘…
ð‘‘
TâˆˆR
d
 oder 
ð‘…
ð»
Ã—
ð‘Š
Ã—
ð¶
R
HÃ—WÃ—C
 etc.

Ziel: Zustand 
âˆ£
ðœ“
âŸ©
âˆ£ÏˆâŸ© (Amplitude/Phase) oder Parameter 
Î˜
Î˜ (Winkel).

Genauigkeit/Skalierung: Welche Wertebereiche sind erlaubt? (Clips, Quantisierung)

Achtung: Wenn du spÃ¤ter messen willst, plane welche Observablen (z. B. Z, ZZ, Pauli-Strings) du brauchst. Das beeinflusst das Circuit-Design.

2) Vorverarbeitung (immer)

2.1 Flatten & Ordnung festlegen

Lege eine stabile Indexierung fest (z. B. row-major fÃ¼r Bilder).

Dokumentiere: Index 
ð‘–
â†”
iâ†” Tensor-Koordinate.

2.2 Skalierung

Angle-Encoding: skaliere 
ð‘¥
x in sinnvollen Bereich, z. B. 
[
âˆ’
1
,
1
]
[âˆ’1,1] und nutze 
ðœƒ
=
ð›¼
ð‘¥
Î¸=Î±x mit 
ð›¼
Î± so, dass Winkel in 
[
âˆ’
ðœ‹
,
ðœ‹
]
[âˆ’Ï€,Ï€] bleiben.

Amplitude-Encoding: normiere 
ð‘¥
x strikt: 
ð‘¥
~
=
ð‘¥
/
âˆ¥
ð‘¥
âˆ¥
2
x
~
=x/âˆ¥xâˆ¥
2
	â€‹

.

Phase-Encoding: mappe 
ð‘¥
x auf 
ðœ™
=
ð›½
ð‘¥
Ï•=Î²x (Range wÃ¤hlen, Aliasing vermeiden).

2.3 Padding auf Potenz von 2 (fÃ¼r Amplituden)

ð‘‘
â€™
=
2
âŒˆ
log
â¡
2
ð‘‘
âŒ‰
dâ€™=2
âŒˆlog
2
	â€‹

dâŒ‰
; padde mit Nullen bis LÃ¤nge 
ð‘‘
â€™
dâ€™.

Notiere Masken/Indizes, damit du spÃ¤ter Auswertung korrekt de-paddest.

Achtung (hÃ¤ufige Fehler):

Ungenau dokumentierte Index-Ordnung â†’ falsches Mapping.

Fehlende L2-Norm vor Amplituden-Encoding â†’ falscher Zustand.

Zu aggressive Clipping/Quantisierung â†’ Informationsverlust.

3) Qubit-Budget & Layout planen

Amplitude-Encoding: 
ð‘›
=
âŒˆ
log
â¡
2
ð‘‘
â€™
âŒ‰
n=âŒˆlog
2
	â€‹

dâ€™âŒ‰ Qubits.

Angle-Encoding: mindestens 
ð‘ž
q Qubits (typisch 
ð‘ž
q = Feature-Slots pro Upload). Wenn 
ð‘‘
â‰«
ð‘ž
dâ‰«q, plane L Re-Upload-Schichten â†’ insgesamt 
ð¿
â‹…
ð‘ž
â‰¥
ð‘‘
Lâ‹…qâ‰¥d (bei einfacher 1:1-Zuordnung; bei Feature-Maps mit Entanglement reduziert sich L).

Kopplung/Connectivity: plane Entanglement-Gatter kompatibel mit Hardware-Topologie (line, heavy-hex, etc.).

Achtung: Mehr Qubits â‰  besser. Rauschen steigt, transpilation wird tiefer.

4) Encoding-Entscheidung treffen (Checkliste)

Wenn du maximale Qubit-Effizienz willst und kannst tiefe Prep tolerieren â†’ Amplitude.

Wenn du StabilitÃ¤t/Einfachheit willst â†’ Angle (+ Re-Upload + Entanglement-BlÃ¶cke).

Wenn Wert nur Index ist â†’ Basis.

Optional: Kombis (z. B. Angle + kontrollierte ZZ-Entangler als Feature-Map).

5) State-Preparation / Parameter-Zuordnung bauen
5A) Angle-Encoding (empfohlen fÃ¼r NISQ)

Schritt A1: Teile 
ð‘¥
x in Chunks der GrÃ¶ÃŸe 
ð‘ž
q (Anzahl Daten-Qubits pro Layer).

Schritt A2: FÃ¼r jeden Chunk 
ð‘
c:

wende auf jedes Qubit 
ð‘—
j eine Rotation an, z. B. 
Ry
(
ð›¼
â‹…
ð‘
ð‘—
)
Ry(Î±â‹…c
j
	â€‹

).

fÃ¼ge Entanglement (z. B. CZ-Kette oder CNOT-Ring) ein â†’ erfasst Korrelationen.

Schritt A3: Wiederhole fÃ¼r L Re-Upload-Layer (neue Chunks oder gleiche Features erneut, je nach Feature-Map-Design).

Validierung: simuliere ideal (Statevector) und prÃ¼fe Erwartungswerte fÃ¼r Test-Observablen.

Achtung:

Zu groÃŸe 
ð›¼
Î± â†’ Winkel-Wrap/PeriodizitÃ¤t.

Zu tiefe Entangler â†’ barren plateaus (Gradienten ~0). Gegenmittel: flache, problem-inspirierte Feature-Maps, layer-weise Training, SPSA.

5B) Amplitude-Encoding (qubit-effizient, aber aufwendig)

Schritt B1: 
ð‘¥
~
=
ð‘¥
/
âˆ¥
ð‘¥
âˆ¥
x
~
=x/âˆ¥xâˆ¥, pad auf 
ð‘‘
â€™
=
2
ð‘›
dâ€™=2
n
.

Schritt B2: Nutze einen State-Prep-Algorithmus (z. B. MÃ¶ttÃ¶nen-Methode / Grover-Rudolph-Variante), der rekursiv Rotationen auf kontrollierten Achsen setzt, um 
âˆ£
ðœ“
âŸ©
=
âˆ‘
ð‘¥
~
ð‘–
âˆ£
ð‘–
âŸ©
âˆ£ÏˆâŸ©=âˆ‘
x
~
i
	â€‹

âˆ£iâŸ© vorzubereiten.

Schritt B3: PrÃ¼fe per Stichproben-Tomographie (oder Overlap-SchÃ¤tzung), ob die Amplituden stimmen (inner product zu Referenz).

KomplexitÃ¤t (wichtig): allgemeines Amplituden-Laden ist O(d) an Rotationen/Kontrollen; auf NISQ schnell tief/rausch-empfindlich.

Achtung:

Genaues L2-Norming und Zahlenbereich (float â†’ WinkelprÃ¤zision).

Signen: Komplexe/negative Werte benÃ¶tigen Phasen-Korrekturen (z. B. zusÃ¤tzliche Z/Phase-Gates).

Noise ruiniert feine Amplituden â†’ ggf. erst komprimieren (PCA/TT-SVD) und dann laden.

5C) Phase-Encoding (optional)

Setze 
ðœ™
ð‘–
=
ð›½
ð‘¥
ð‘–
Ï•
i
	â€‹

=Î²x
i
	â€‹

 und appliziere kontrollierte Phasen auf 
âˆ£
ð‘–
âŸ©
âˆ£iâŸ© (oder direkte Rz auf adressierten Qubits bei Feature-Maps).

Gut, wenn Interferenz-muster wichtiger sind als Amplituden.

6) SpezialfÃ¤lle: Bilder & Convolution-Kerne

Bilder (HÃ—WÃ—C):

Flatten mit Struktur (erst Pixelposition â†’ dann Kanal), oder nutze zwei Register: Positions-Register (fÃ¼r |iâŸ©) + Wert-Register (Winkel/Amplitude).

Downsampling/Kompression (PCA, DCT, Patch-Averaging) vor Amplituden-Encoding.

Kerne (kÃ—kÃ—C): kleine Tensoren eignen sich gut fÃ¼r Angle-Encoding als Parameter in wenigen Qubits; entangling-Pattern kann Nachbarschaft im Kernel widerspiegeln.

Achtung: Erhalte lokale Strukturâ€”vermeide wahlloses Flatten, wenn du mit Entanglement Nachbarschaften modellieren willst.

7) Mess-Design (wie prÃ¼fst du â€žrichtig geladenâ€œ)

Definiere Observablen 
ð‘‚
O (z. B. ZâŠ—Z, XâŠ—Iâ€¦); Ziel ist 
âŸ¨
ðœ“
âˆ£
ð‘‚
âˆ£
ðœ“
âŸ©
âŸ¨Ïˆâˆ£Oâˆ£ÏˆâŸ©.

FÃ¼r Amplituden-Encoding kannst du punktuell Amplitude/Probability-Checks machen (zÃ¤hle HÃ¤ufigkeiten der |iâŸ©-Outcomes).

FÃ¼r Angle-Encoding vergleiche Feature-zu-Winkel-Mapping Ã¼ber kontrollierte Test-Circuits (z. B. identische Layer klassisch simulieren und Erwartungswerte matchen).

Achtung: Shot-Noise (endliche Mess-wiederholungen). Nutze genÃ¼gend shots und readout-Mitigation (einfaches Kalibrieren).

8) Korrektheits- und Robustheits-Checks

Roundtrip-Test (soweit mÃ¶glich): klassisch â†’ encode â†’ (inverse Prep) â†’ klassisch rekonstruieren; prÃ¼fe MSE.

Skalierungs-Sweep: teste verschiedene 
ð›¼
,
ð›½
Î±,Î² (Winkelskalen) und Layers 
ð¿
L.

Noise-Sweep: simuliere mit Rauschmodell; prÃ¼fe, ab wann Metriken kippen.

A/B Feature-Maps: vergleiche einfache Ry-Map vs. z. B. ZZ-FeatureMap (zusÃ¤tzliche Wechselwirkungsterme).

9) Performance-Tuning (wennâ€™s groÃŸ wird)

Kompression vor Encoding: PCA/Random-Features/TT-SVD auf 
ð‘¥
x, dann Angle-Encoding.

Tensor-Netzwerk-Tricks: Tree/Tensor-Train-basierte State-Prep reduziert Tiefe fÃ¼r strukturierte Daten.

Data-Reuploading statt groÃŸe n: gleiche Qubits, mehrere Re-Upload-Layers (gute Praxis auf NISQ).

Sparsame Amplituden: wenn 
ð‘¥
x spÃ¤rlich ist, nutze selektive State-Prep (nur relevante |iâŸ©-Ã„ste).

Block-Encoding (fortgeschritten): Matrix 
ð´
A als Teil eines UnitÃ¤rs einbetten; praktikabel vor allem fÃ¼r sparsame oder low-rank 
ð´
A. Dicht & groÃŸ ist teuer.

Achtung (harte Grenzen): Allgemeines, schnelles Laden beliebiger groÃŸer Tensoren ist offen/teuer; â€žschnelles QRAMâ€œ ist nicht praktisch verfÃ¼gbar.

10) Mini-Vorlagen (pseudocode-artig)

Angle-Encoding mit Re-Upload (Skizze):

# Inputs: x (len d), q qubits per layer, L layers, scale alpha
chunks = split(x, size=q, pad_with=0)
for layer in range(L):
    for j in range(q):
        Ry(alpha * chunks[layer][j]) on qubit j
    Entangle(qubits)  # e.g., CZ chain


Amplitude-Encoding (Skizze):

# Input: x (len d), pad to d' = 2^n, normalize
x = pad_to_power_of_two(x)
x = x / l2norm(x)
StatePrep_Mottonen(x)  # builds recursive controlled rotations


Phase-Encoding (Skizze):

for i in addressed_indices:
    apply controlled-Phase( beta * x[i] ) to basis |i>
# or Rz(beta * feature) on selected qubits in a feature-map

11) Typische Fallschritte (kurz & konkret)

Keine saubere Normierung bei Amplitude-Encoding â†’ vÃ¶llig falscher Zustand.

Zu tiefe Circuits â†’ barren plateaus; starte flach, Layer-weise erweitern.

Falsche Feature-Skalen â†’ Winkel-SÃ¤ttigung/PeriodizitÃ¤t.

Zu wenige Shots â†’ verrauschte Metriken, falsche SchlÃ¼sse.

Ignorierte Hardware-Topologie â†’ Transpiler blÃ¤st Tiefe auf.

Strukturverlust beim Flatten â†’ schlechtere Lernleistung; lieber strukturierte Feature-Maps.

12) Entscheidungsbaum (kurz)

d groÃŸ, Qubits knapp, Rauschen okay? â†’ Angle + Re-Upload + Entangler.

d groÃŸ, du brauchst volle Vektorinhalte kompakt? â†’ Amplitude, aber vorher komprimieren.

Diskrete Indizes/Klassen? â†’ Basis (oder Angle auf kleinen One-Hot-Projektionen).

Interferenz-lastig? â†’ Phase (oder kombinierte Feature-Map).

## Fortschritt (26.08.2025)

- [x] Spezifikation â€” `specs/tensor_spec.yaml`, `scripts/smoke_validate_spec.py`
- [x] Vorverarbeitung â€” `qnn/preprocess.py`, Demos/Tests: `scripts/demo_preprocess.py`, `tests/test_preprocess.py`
- [x] Planung â€” Angle q,L in `qnn/planning.py`
- [x] Encoding-Pfade â€” `qnn/circuits.py`; Demos: `scripts/demo_angle_sim.py`, `scripts/demo_phase_sim.py`, `scripts/demo_amplitude_sim.py`
- [x] SpezialfÃ¤lle/Demos â€” `scripts/demo_image_angle.py`, `scripts/demo_kernel_grid.py`
- [x] Mess-Design â€” `scripts/demo_measurements.py`
- [x] Robustheit/Noise â€” `scripts/demo_noise_angle.py`
- [x] Transpilation/Backend â€” `qnn/transpile.py`, `scripts/demo_transpile_angle.py`
- [x] Sweeps & Plots â€” `scripts/sweep_alpha_L.py`, `scripts/sweep_noise.py`, `scripts/plot_sweeps.py`, Artefakte: `reports/*.png`
- [x] VQA & Classifier â€” `qnn/vqa.py`, Demos: `scripts/demo_vqa_spsa.py`, `scripts/demo_reupload_classifier.py`
- [x] Training â€” `scripts/train_reupload_classifier.py` (Logs â†’ `reports/train_reupload_classifier.json`, Params â†’ `reports/reupload_params.json`)
- [x] CLI/UX & Config â€” Konsolenbefehle `qnn-train`/`qnn-predict`; `qnn/config.py`, `configs/example.yaml`, `qnn/models.py`, `scripts/predict_reupload_classifier.py`
- [x] Tests â€” Pipeline/Models/Config: 6/6 grÃ¼n
- [x] CI â€” GitHub Actions: Spec/Tests/Demos + Smoke Train/Predict (`.github/workflows/ci.yml`)

NÃ¤chste sinnvolle Schritte (kurz)
- Prediction-Export (`--export json|csv`) und reproduzierbares `--seed` + Logging.
- Hardware-Targeting: Basisgates/Coupling in Demos/CI prÃ¼fen, Transpile-Metriken festhalten.
- Realdaten-Loader (CSV/NumPy) + Evaluierung (Accuracy/AUC) und Ergebnis-Reports.