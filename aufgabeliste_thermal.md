# Projekt: Transformer-to-TSU Adapter
*Hybrid Deterministic-Probabilistic Inference mit Thermodynamic Computing*

Dieses Projekt zielt darauf ab, vortrainierte Transformer-Modelle (z.B. BERT, GPT) auf thermodynamische Hardware (Extropic TSU) zu portieren, indem Gewichte in Energielandschaften √ºbersetzt werden.

---

## üì¶ Phase 1: THRML Setup & Verst√§ndnis
*Extropic hat bereits eine Open-Source-Bibliothek: `thrml` (JAX-basiert)*

- [x] **THRML installieren & Dokumentation studieren**
    - [x] `pip install thrml` ausf√ºhren.
    - [x] Beispiel-Notebooks durchgehen (`examples/02_spin_models.ipynb`).
    - [x] Verstehen: `SpinNode`, `Block`, `IsingEBM`, `IsingSamplingProgram`.
- [x] **Kern-Konzepte verinnerlichen**
    - [x] **Energy-Based Models (EBM):** Wie werden Energiefunktionen definiert?
    - [x] **Block Gibbs Sampling:** Wie funktioniert der `sample_states`-Aufruf?
    - [x] **Faktoren vs. Sampler:** `SpinEBMFactor` (definiert Interaktionen) vs. `SpinGibbsConditional` (sampled).
- [x] **Mini-Test: Einfaches Ising-Modell**
    - [x] 5-Node-Chain aus der README nachbauen.
    - [x] Samples ziehen und visualisieren (Spin-Korrelationen).

## üß© Phase 2: Mathematische √úbersetzung (Transformer ‚Üí EBM)
*Wie √ºbersetzen wir Transformer-Gewichte in eine Energielandschaft?*

- [x] **Theorie: Linear Layer als RBM**
    - [x] Paper/Blog lesen: "Restricted Boltzmann Machines and Deep Learning".
    - [x] Verstehen: $E(v, h) = -v^T W h - b^T h$ (Visible-Hidden-Kopplung).
    - [x] Frage kl√§ren: Wie wird ein deterministischer Forward-Pass ($y = Wx + b$) in bedingtes Sampling √ºbersetzt?
- [x] **Theorie: Attention als Energie**
    - [x] Softmax $\propto \exp(QK^T)$ ist eine Boltzmann-Verteilung.
    - [x] Wie kann man "Attention-Indizes" samplen statt exakt zu berechnen?
- [x] **Prototyp: Toy-Beispiel (ohne Transformer)**
    - [x] Ein 2-Layer MLP (Linear+ReLU+Linear) als `SpinEBMFactor` ausdr√ºcken.
    - [x] Vergleich: Deterministischer Forward vs. Sampled Forward.

## ‚ö° Phase 3: Adapter-Implementierung
*Transformer-Gewichte ‚Üí THRML Faktoren*

- [x] **Klasse `TransformerToThermalAdapter`**
    - [x] Methode `convert_linear_to_factor(nn.Linear)` ‚Üí gibt `SpinEBMFactor` zur√ºck.
    - [x] Gewichts-Skalierung: Transformer-Weights normalisieren (z.B. auf [-1, 1]).
    - [x] Temperature-Parameter: Steuert "Kreativit√§t" des Samplings.
- [x] **Integration mit THRML**
    - [x] Konstruiere `BlockGibbsSpec` aus Transformer-Architektur.
    - [x] Definiere `free_blocks` (die Layer, die gesampelt werden).
    - [x] Definiere `clamped_blocks` (Input-Embeddings als fixierte Bedingung).
- [x] **Forward-Pass Ersetzung**
    - [x] Ersetze `model.forward(x)` durch `sample_states(...)` (via `ThermalLinear`).
    - [x] R√ºckgabe: Ensemble von Outputs (mehrere Samples) statt einem deterministischen Wert (Mean returned).
    - [x] Utility `replace_linear_layers` implementiert.

## üöÄ Phase 4: Integration & Demo
*Zusammenf√ºgen der Teile*

- [x] **End-to-End Demo**
    - [x] Lade ein kleines PyTorch Modell (z.B. MNIST MLP).
    - [x] Konvertiere es mit `replace_linear_layers`.
    - [x] F√ºhre Inference durch und vergleiche Accuracy (Deterministic vs. Thermal).
- [x] **Attention-Integration**
    - [x] Implementiere `ThermalAttention` (analog zu `ThermalLinear`).
    - [x] Teste mit einem kleinen Transformer-Block (Core Attention Mechanism verified).

## üõ†Ô∏è Phase 5: Engineering & Optimierung (Refactoring)
*Vom Prototyp zur skalierbaren L√∂sung*

- [x] **Refactor `ThermalLinear`: Input Fidelity**
    - [x] Problem: Aktuell wird Input hart auf `x > 0` (bin√§r) gesetzt. Informationsverlust.
    - [x] L√∂sung: "Effective Fields" nutzen. $B_{eff} = Wx + b$ in PyTorch berechnen.
    - [x] TSU als stochastische Aktivierungsfunktion nutzen (statt vollem Ising-Graph f√ºr MatMul).
- [x] **Refactor `ThermalAttention`: Performance**
    - [x] Problem: Graph wird bei jedem Forward-Pass neu gebaut (Python Loop Overhead).
    - [x] L√∂sung: JAX `jit` Kompilierung f√ºr statische Graphen nutzen.
- [x] **Scalability & Vectorization**
    - [x] Problem: Python-Loops in `convert_linear_layer` (O(N*M)).
    - [x] L√∂sung: `itertools.product` und NumPy Flattening f√ºr vektorisierte Edge-Erstellung.
    - [x] Test: 1M Edges in ~0.2s konstruiert.

## üîå Phase 6: Hardware Realism
*Vorbereitung auf physische Constraints*

- [x] **Sparsity Support**
    - [x] Problem: Fully Connected Graphen sind auf Hardware schwer abbildbar.
    - [x] L√∂sung: `sparsity_threshold` einf√ºhren. Nur Gewichte $|w| > \theta$ werden als Edges realisiert.
    - [x] Optimierung: Sparse-Construction (nur relevante Edges iterieren) statt Dense-Construction.

## üèóÔ∏è Phase 7: Engineering Refactoring
*Vom Prototyp zur robusten Architektur*

- [x] **RNG State Management**
    - [x] Problem: `np.random` (Dirty Hack) bricht Reproduzierbarkeit.
    - [x] L√∂sung: `ThermalContext` Klasse eingef√ºhrt, die JAX PRNGKeys deterministisch verwaltet.
- [x] **Central Context & Annealing**
    - [x] Problem: Fragmentierte Adapter-Instanzen verhindern globale Steuerung.
    - [x] L√∂sung: `ThermalContext` h√§lt globalen State (Temperatur).
    - [x] Feature: Globales Annealing (T_start -> T_end) √ºber alle Layer hinweg m√∂glich.
    - [x] Test: `scripts/test_global_annealing.py` verifiziert Steuerung.
- [x] **Backward Pass (Training)**
    - [x] Problem: Sampling ist nicht differenzierbar. Training unm√∂glich.
    - [x] L√∂sung: `ThermalActivationFunction` mit Straight-Through Estimator (STE) implementiert.
    - [x] Test: `scripts/test_training.py` zeigt erfolgreiches Lernen (Weight Update via SGD).

## üìù Dokumentation & API
- [ ] Docstrings f√ºr alle Adapter-Methoden.
- [ ] Beispiel-Notebook `demo_thermal_transformer.ipynb` erstellen.
